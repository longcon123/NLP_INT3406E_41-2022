{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocmcqtWyJEch"
      },
      "source": [
        "# NLP: INT3406E_41 (2022) - PGS.TS: Nguyen Phuong Thai\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "PROGRAMMING ASSIGNMENT 1: WORD SIMILARITY AND SEMANTIC RELATION CLASSIFICATION\n",
        "#Vũ Hoàng Long 19020036\n",
        "#Trần Long Dũng 19021321"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKHBBzOnEItQ",
        "outputId": "2275b495-7918-4e1d-e40f-c099bdadfcd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Cloning into 'Word-Similarity'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Total 196 (delta 0), reused 0 (delta 0), pack-reused 196\u001b[K\n",
            "Receiving objects: 100% (196/196), 5.81 MiB | 10.64 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!git clone https://github.com/NLP-Projects/Word-Similarity.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIV25hQsFwGo"
      },
      "source": [
        "# EX1: Cosine similarity:\n",
        "Given pre-trained embeddings of Vietnamese words, implement a function for calculating cosine similarity between word pairs. Test your program using word pairs in ViSim-400 dataset (in directory Datasets/ViSim-400)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD1zHlUcHaRT"
      },
      "source": [
        "**Dowload pretrained Word2vec Dataset:**\n",
        "Link download pre-trained word2vec model \n",
        "https://drive.google.com/open?id=1z1IDKaZuJXw5g7Yebr1GDq2UfvVR_aGx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SVrwxLFFiUt",
        "outputId": "1c49834c-0fb9-4278-a328-2fbbb0ede366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z1IDKaZuJXw5g7Yebr1GDq2UfvVR_aGx\n",
            "To: /content/W2V_150.txt\n",
            "100% 119M/119M [00:00<00:00, 180MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1z1IDKaZuJXw5g7Yebr1GDq2UfvVR_aGx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoK3yqEuUf3w"
      },
      "source": [
        "**Extract pretrained word2vec data into a Dictionary for searching embedded vectors by key work**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N8HQJoWaEn0"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "vec_data_file = '/content/W2V_150.txt'\n",
        "def word2vec_dictionary(data_file):\n",
        "  i = 0\n",
        "  emb_vec_dict = {}\n",
        "  with open(data_file, 'r') as fd:\n",
        "      reader = csv.reader(fd)\n",
        "      for row in reader:\n",
        "        split_data = row[0].split(' ')\n",
        "        word = split_data[0]\n",
        "        emb_vec = []\n",
        "        for v in split_data[2:-1]:\n",
        "          emb_vec.append(float(v))\n",
        "        emb_vec_dict[word] = emb_vec\n",
        "  return emb_vec_dict\n",
        "word2vec = word2vec_dictionary(vec_data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uooL5XGD5mon",
        "outputId": "67cac575-16bc-4208-ac2c-1fa117963d10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150 150\n"
          ]
        }
      ],
      "source": [
        "print(len(word2vec['chó']), len(word2vec['mèo']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-2L9AZpKOTg"
      },
      "source": [
        "**Define the cosine_similarity function for calculating the cosine similarity between 2 vecs:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzpVumVwFi9C",
        "outputId": "5aca74fe-625a-443f-c1af-16a2e916e358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9844319658134832\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import spatial\n",
        "\n",
        "a = [1,2,3,4]\n",
        "b = [5,6,7,8]\n",
        "def cosine_similarity_cal(v1, v2):\n",
        "  return (2 - spatial.distance.cosine(v1, v2))/2\n",
        "print(cosine_similarity_cal(a,b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po3n8aS9lw0E"
      },
      "source": [
        "**Calculate the cosine similarity between 2 word by vec:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yiqucz42Eld1",
        "outputId": "8ec9cb39-d853-4e49-b0e1-04695b4708d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This compair word was not found in dictionary!\n",
            "Pair-Word: biến - ngập, Cosine-similarity: 0.497543830265165\n",
            "Pair-Word: nhà_thi_đấu - nhà, Cosine-similarity: 0.5412615916460588\n",
            "Pair-Word: động - tĩnh, Cosine-similarity: 0.6385429799341388\n",
            "Pair-Word: khuyết - ưu, Cosine-similarity: 0.5883993141781336\n",
            "This compair word was not found in dictionary!\n",
            "Pair-Word: thủ_pháp - biện_pháp, Cosine-similarity: 0.7011830645971531\n",
            "Pair-Word: kết_duyên - thành_hôn, Cosine-similarity: 0.7315042010070361\n",
            "Pair-Word: cấp_tiến - bảo_thủ, Cosine-similarity: 0.6284735044498062\n",
            "Pair-Word: nước_lớn - nguy_hiểm, Cosine-similarity: 0.5925960112010581\n"
          ]
        }
      ],
      "source": [
        "data_file = '/content/Word-Similarity/datasets/ViSim-400/Visim-400.txt'\n",
        "#data = f = open(data_file, \"r\")\n",
        "import csv\n",
        "i = 0\n",
        "n_compair = 10\n",
        "with open(data_file, 'r') as fd:\n",
        "    reader = csv.reader(fd)\n",
        "    #pair_words = []\n",
        "    for row in reader:\n",
        "      split_data = row[0].split('\\t')\n",
        "      word1 = split_data[0]\n",
        "      word2 = split_data[1]\n",
        "      try:\n",
        "        vec1 = np.array(word2vec[word1])\n",
        "        vec2 = np.array(word2vec[word2])\n",
        "        print(\"Pair-Word: {} - {}, Cosine-similarity: {}\".format(word1, word2, cosine_similarity_cal(vec1, vec2)))\n",
        "      except:\n",
        "        print('This compair word was not found in dictionary!')\n",
        "      i+=1\n",
        "      if i == n_compair:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XBBHMWyyXLp"
      },
      "source": [
        "# EX2: K-nearest words: Given a word w, find k most-similar words of w using the function implemented in question 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVWu9VEP_d4q"
      },
      "source": [
        "**Find k nearest text function:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYe3sjGgyg4K",
        "outputId": "33a7a87c-ed25-4d36-a9a4-cb53ee9a18a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input w word: báo_đốm\n",
            "3 nearest: ['sư_tử', 'hổ', 'mèo']\n"
          ]
        }
      ],
      "source": [
        "def k_nearest_neighbor(w, v, k = 1):\n",
        "  similar_vec = [] #create a similar vec list for choosing a nearest vector\n",
        "  map_v2w = {} #create a dictionary for mapping the vector to word after find the k nearest\n",
        "  k_nearest_word = [] #create a k-nearest list word for return the near rest word list \n",
        "  for i in range(len(v)):\n",
        "    w_vec = word2vec[w] #search the embedding vector of the word\n",
        "    word_vec = word2vec[v[i]] #search the embedding vector of the word\n",
        "    csc = cosine_similarity_cal(w_vec, word_vec) #calculate the cosine similarity\n",
        "    similar_vec.append(csc) #append similar score to list for searching the nearest\n",
        "    map_v2w[csc] = v[i] #adding the key: vector and value: word to dictionary for quering the word which is nearest\n",
        "  similar_vec.sort() #sorting by ascending order\n",
        "  k_nearest_vec = similar_vec[-k:] #quering the k-nearest vector\n",
        "  for vec in k_nearest_vec:\n",
        "    word = map_v2w[vec] #mapping the k-nearest vector to word\n",
        "    k_nearest_word.append(word)\n",
        "  return k_nearest_word\n",
        "\n",
        "#Testing function\n",
        "k = 3\n",
        "w = \"báo_đốm\"\n",
        "v = [\"nước_lớn\", \"sông\", \"nhà_thi_đấu\", \"kết_duyên\", \"mèo\", \"thủy\", \"nguy_hiểm\", \"sóng\", \"hổ\", \"sư_tử\"]\n",
        "print(\"Input w word: {}\".format(w))\n",
        "print(\"{} nearest:\".format(k), k_nearest_neighbor(w, v, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNDqw0i9YopJ"
      },
      "source": [
        "# EX3: Synonym-antonym classification: Implement a Logistic Regression classifier or a Multi-layer Perceptron classifier for distinguishing synonyms and antonyms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfEX4eR7vIhR"
      },
      "source": [
        "**Extract 301 Vector: (150 each word) and Sim Score (1 each pair-word) of Synonym and Antonym dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aEXhwRdvITd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "ant_file = '/content/Word-Similarity/antonym-synonym set/Antonym_vietnamese.txt'\n",
        "syn_file = '/content/Word-Similarity/antonym-synonym set/Synonym_vietnamese.txt'\n",
        "\n",
        "css = []\n",
        "type_of_word = []\n",
        "dict_dataset = {}\n",
        "dict_lists = {}\n",
        "for i in range(1,301):\n",
        "  dict_lists['l'+str(i)] = []\n",
        "with open(ant_file, 'r') as fd:\n",
        "    for row in fd: \n",
        "      split_word = row.split(' ')\n",
        "      try: #try if dictionary have that word\n",
        "        word1 = split_word[0].strip()\n",
        "        word2 = split_word[1].strip()\n",
        "        vec1 = np.array(word2vec[word1])\n",
        "        vec2 = np.array(word2vec[word2])\n",
        "        vec12 = np.concatenate((vec1, vec2))\n",
        "        css.append(cosine_similarity_cal(vec1, vec2))\n",
        "        type_of_word.append(0)\n",
        "        for i in range(300):\n",
        "            dict_lists['l'+str(i+1)].append(vec12[i])\n",
        "      except:\n",
        "        pass\n",
        "with open(syn_file, 'r') as fd:\n",
        "    for row in fd:\n",
        "      split_word = row.split(' ')\n",
        "      try: #try if dictionary have that word\n",
        "        word1 = split_word[0].strip()\n",
        "        word2 = split_word[1].strip()\n",
        "        vec1 = word2vec[word1]\n",
        "        vec2 = word2vec[word2]\n",
        "        vec12 = np.concatenate((vec1, vec2))\n",
        "        css.append(cosine_similarity_cal(vec1, vec2))\n",
        "        type_of_word.append(1)\n",
        "        for i in range(300):\n",
        "            dict_lists['l'+str(i+1)].append(vec12[i])\n",
        "      except:\n",
        "        pass\n",
        "dict_dataset['CSS'] = css\n",
        "dict_dataset['Type'] = type_of_word\n",
        "for i in range(300):\n",
        "  dict_dataset['vec'+str(i+1)] = dict_lists['l'+str(i+1)]\n",
        "df = pd.DataFrame(dict_dataset)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "df_no_type = df.drop('Type', axis=1)\n",
        "features = df_no_type.to_numpy()\n",
        "labels = df['Type'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fSzt0yA2ub3",
        "outputId": "cbcbcf3e-4250-4563-c4ff-2f4e24bf6866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           CSS  Type      vec1      vec2      vec3      vec4      vec5  \\\n",
            "7965  0.649490     1 -0.969126  2.503197  1.606285  1.267443  0.704963   \n",
            "750   0.652996     0  0.032384  0.886631 -2.745771 -2.501953  3.117980   \n",
            "6738  0.548607     1  0.141208  0.497018 -0.325516 -1.347731 -0.125896   \n",
            "5321  0.884776     1  0.613247  2.044406  0.722219  0.670574  0.383656   \n",
            "2690  0.807124     1  0.819285  0.143123  1.499029 -2.458985  1.112694   \n",
            "...        ...   ...       ...       ...       ...       ...       ...   \n",
            "3660  0.669240     1 -1.534978  0.975442 -1.254490  1.011634  1.062638   \n",
            "6505  0.718027     1  1.242174  0.615960  0.674266  0.535518 -2.756795   \n",
            "3890  0.766592     1  2.369785  1.884284  1.460598  0.756433  1.119153   \n",
            "7059  0.685703     1  3.048424  3.572027  1.315129  0.211011  1.833323   \n",
            "5409  0.557399     1  1.249571 -0.058011  0.383467  1.799968 -0.748270   \n",
            "\n",
            "          vec6      vec7      vec8  ...    vec291    vec292    vec293  \\\n",
            "7965  2.762822  0.602556  0.602971  ...  0.638103 -0.026125 -0.384844   \n",
            "750  -1.352366 -1.635234  0.322161  ... -0.596496 -0.405011 -1.181035   \n",
            "6738  0.951056 -2.361040  0.746338  ... -2.076138  1.298523  1.243676   \n",
            "5321  0.576224  3.274584  1.884542  ... -0.239349 -0.477793 -0.486795   \n",
            "2690  2.439281  0.676865  0.978516  ...  0.111866  0.806488 -0.709872   \n",
            "...        ...       ...       ...  ...       ...       ...       ...   \n",
            "3660  0.488028 -1.219087 -0.307046  ... -1.583651  1.165088  0.080224   \n",
            "6505  0.267821 -0.790134  0.926398  ... -0.940993 -2.173538 -0.982833   \n",
            "3890  0.373873  0.746209 -0.763716  ...  0.496587 -0.888519 -1.483051   \n",
            "7059  1.502006  0.237999  2.509364  ... -0.774407 -1.773972 -0.834731   \n",
            "5409  0.766373 -2.422134 -1.319078  ...  0.645161 -0.919103 -0.101827   \n",
            "\n",
            "        vec294    vec295    vec296    vec297    vec298    vec299    vec300  \n",
            "7965 -0.067757  0.634515 -0.283422  1.392439  0.864599 -0.994049  0.371535  \n",
            "750  -0.218128 -0.541395 -1.092625  1.021436  0.989406 -0.481289  0.472773  \n",
            "6738  0.278741  0.066992 -1.685823 -1.274909  0.247475  1.534576  0.546159  \n",
            "5321  0.117276 -0.729451 -0.824448  0.102868  1.984337 -0.503162 -1.214584  \n",
            "2690  1.432051 -0.743390 -1.222194 -1.868476 -1.524867  1.193867  1.104820  \n",
            "...        ...       ...       ...       ...       ...       ...       ...  \n",
            "3660 -0.664798 -0.417163 -1.789477  0.105530  1.671598 -0.081331  0.092257  \n",
            "6505  2.646256 -0.688908 -2.540347  0.191425 -1.320110  0.672462  0.347227  \n",
            "3890  0.167759 -0.796795  1.542394  1.183237 -1.340280  1.069114 -0.689628  \n",
            "7059  0.981569 -2.366190 -2.456630 -0.645586  2.411297  0.967562  1.607403  \n",
            "5409 -3.238950 -0.051640 -1.124953  0.065759  0.371491 -0.455565 -1.288017  \n",
            "\n",
            "[7998 rows x 302 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR_Zw0IepJf3"
      },
      "outputs": [],
      "source": [
        "df.to_csv('Word_pair_by_Vec_Data.csv', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NXLa2Jx21YQ"
      },
      "source": [
        "**Split data for training and testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfxyR6fNvqE7",
        "outputId": "c8313126-c407-4539-fff3-cc20074cde29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training records: 5598\n",
            "Val records: 2400\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(features, labels, test_size=0.30)\n",
        "print('Training records:', Y_train.size)\n",
        "print('Val records:', Y_val.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing training with sklearn nn MLP classifier**"
      ],
      "metadata": {
        "id": "rGBQRzdYNr89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6VMZ7e65EKG",
        "outputId": "696ccf43-71a0-4a14-eb2b-e1effa3ed4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.40900404\n",
            "Iteration 2, loss = 0.20151944\n",
            "Iteration 3, loss = 0.09910788\n",
            "Iteration 4, loss = 0.04263549\n",
            "Iteration 5, loss = 0.02712714\n",
            "Iteration 6, loss = 0.01565339\n",
            "Iteration 7, loss = 0.01673839\n",
            "Iteration 8, loss = 0.01415252\n",
            "Iteration 9, loss = 0.02768228\n",
            "Iteration 10, loss = 0.04415880\n",
            "Iteration 11, loss = 0.03170392\n",
            "Iteration 12, loss = 0.02031861\n",
            "Iteration 13, loss = 0.01063132\n",
            "Iteration 14, loss = 0.00576693\n",
            "Iteration 15, loss = 0.00480739\n",
            "Iteration 16, loss = 0.00489751\n",
            "Iteration 17, loss = 0.00257582\n",
            "Iteration 18, loss = 0.00170721\n",
            "Iteration 19, loss = 0.00140536\n",
            "Iteration 20, loss = 0.00111618\n",
            "Iteration 21, loss = 0.00083685\n",
            "Iteration 22, loss = 0.00070379\n",
            "Iteration 23, loss = 0.00076875\n",
            "Iteration 24, loss = 0.00061915\n",
            "Iteration 25, loss = 0.00084165\n",
            "Iteration 26, loss = 0.00083560\n",
            "Iteration 27, loss = 0.00060354\n",
            "Iteration 28, loss = 0.00077761\n",
            "Iteration 29, loss = 0.00129941\n",
            "Iteration 30, loss = 0.00095950\n",
            "Iteration 31, loss = 0.00088255\n",
            "Iteration 32, loss = 0.00088450\n",
            "Iteration 33, loss = 0.00116740\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(64, 32), learning_rate_init=0.01,\n",
              "              random_state=1, verbose=True)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import MLPClassifer \n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create model object\n",
        "clf = MLPClassifier(hidden_layer_sizes=(64,32),\n",
        "                    random_state=1,\n",
        "                    verbose=True,\n",
        "                    learning_rate_init=0.01)\n",
        "\n",
        "# Fit data onto the model\n",
        "clf.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST THE MODEL WITH VISIM400 and print OUTPUT**"
      ],
      "metadata": {
        "id": "gIGkVNZiG73Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucILe4cozwkO",
        "outputId": "66004c6d-5648-46d5-fc23-d2eb6a254acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:\n",
            "___________________________________________________________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96       172\n",
            "           1       0.94      0.98      0.96       154\n",
            "\n",
            "    accuracy                           0.96       326\n",
            "   macro avg       0.96      0.96      0.96       326\n",
            "weighted avg       0.96      0.96      0.96       326\n",
            "\n",
            "____________________________________________________________________\n",
            "1 is Synonym and 0 is Antonym, Output test Visualize:\n",
            "____________________________________________________________________\n",
            "          Word1     Word2  Cosine Similarity  Predicted  True Value\n",
            "0     khoái_lạc   nỗi_đau           0.622883          1           0\n",
            "1       yếu_kém  sức_mạnh           0.585686          0           0\n",
            "2    thanh_danh    ô_nhục           0.544493          0           0\n",
            "3           dây       dọc           0.570831          1           1\n",
            "4            ba        me           0.475859          0           0\n",
            "..          ...       ...                ...        ...         ...\n",
            "321          mẹ        bu           0.555782          1           1\n",
            "322         hòm     thùng           0.681865          1           1\n",
            "323        cuối       đầu           0.821198          0           0\n",
            "324        đống        lô           0.663529          1           1\n",
            "325       chồng        vợ           0.859242          0           0\n",
            "\n",
            "[326 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "test_file = '/content/Word-Similarity/datasets/ViCon-400/400_noun_pairs.txt'\n",
        "#data = f = open(data_file, \"r\")\n",
        "import csv\n",
        "k = 0\n",
        "syn_ant = {\n",
        "    \"ANT\": 0,\n",
        "    \"SYN\": 1\n",
        "}\n",
        "n_compair = 1\n",
        "test_data = []\n",
        "vec = []\n",
        "css = []\n",
        "type_of_word = []\n",
        "dict_testset = {}\n",
        "dict_lists = {}\n",
        "p_word1 = []\n",
        "p_word2 = []\n",
        "for i in range(1,301):\n",
        "  dict_lists['l'+str(i)] = []\n",
        "with open(test_file, 'r') as fd:\n",
        "      for row in fd:\n",
        "        if k >= 1:\n",
        "          split_data = row.split('\\t')\n",
        "          try: #try if dictionary have that word\n",
        "            word1 = split_data[0].strip()\n",
        "            word2 = split_data[1].strip()\n",
        "            types = split_data[2].strip()\n",
        "            #print(word1, word2, syn_ant[types])\n",
        "            vec1 = np.array(word2vec[word1])\n",
        "            vec2 = np.array(word2vec[word2])\n",
        "            type_of_word.append(syn_ant[types])\n",
        "            vec12 = np.concatenate((vec1, vec2))\n",
        "            css.append(cosine_similarity_cal(vec1, vec2))\n",
        "            p_word1.append(word1)\n",
        "            p_word2.append(word2)\n",
        "            for i in range(300):\n",
        "              dict_lists['l'+str(i+1)].append(vec12[i])\n",
        "          except:\n",
        "            pass\n",
        "        k+=1\n",
        "dict_testset['Word1'] = p_word1\n",
        "dict_testset['Word2'] = p_word2\n",
        "dict_testset['CSS'] = css\n",
        "dict_testset['Type'] = type_of_word\n",
        "for i in range(300):\n",
        "  dict_testset['vec'+str(i+1)] = dict_lists['l'+str(i+1)]\n",
        "df = pd.DataFrame(dict_testset)\n",
        "df_only_input = df.drop(['Word1','Word2','Type'], axis=1)\n",
        "# create test data\n",
        "test = df_only_input.to_numpy()\n",
        "test_labels = df['Type'].to_numpy()\n",
        "# prediction\n",
        "predicted_y = clf.predict(test)\n",
        "#print(X_val)\n",
        "#print(predicted_y)\n",
        "#Print output\n",
        "print(\"Accuracy:\")\n",
        "print(\"___________________________________________________________________\")\n",
        "print(metrics.classification_report(test_labels, predicted_y))\n",
        "output_dict = {\"Word1\" : p_word1,\n",
        "               \"Word2\" : p_word2,\n",
        "               \"Cosine Similarity\": css,\n",
        "               \"Predicted\": predicted_y,\n",
        "               \"True Value\": type_of_word\n",
        "               }\n",
        "out_test = pd.DataFrame(output_dict)\n",
        "print(\"____________________________________________________________________\")\n",
        "print(\"1 is Synonym and 0 is Antonym, Output test Visualize:\")\n",
        "print(\"____________________________________________________________________\")\n",
        "print(out_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PorpkMm2n3YT"
      },
      "source": [
        "**Implement the MLP from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAZTkZWLg8LK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "class MultiLayerPerceptron():\n",
        "  def __init__(self, sizes, epochs=10, l_rate=0.001):\n",
        "    self.sizes = sizes\n",
        "    self.epochs = epochs\n",
        "    self.l_rate = l_rate\n",
        "\n",
        "    self.params = self.initialization()\n",
        "  \n",
        "  def sigmoid(self, x, der=False):\n",
        "    if der:\n",
        "      return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "    return 1/(1+np.exp(-x))\n",
        "  \n",
        "  def softmax(self, x, der=False):\n",
        "        exps = np.exp(x - x.max())\n",
        "        if der:\n",
        "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "  def ReLu(self, x, der=False):\n",
        "    if der:\n",
        "      return np.heaviside(x, 1)\n",
        "    return np.maximum(x, 0)\n",
        "    \n",
        "  def initialization(self):\n",
        "    input_layer = self.sizes[0]\n",
        "    hidden_1 = self.sizes[1]\n",
        "    hidden_2 = self.sizes[2]\n",
        "    output_layer = self.sizes[3]\n",
        "\n",
        "    params = {\n",
        "        'W1':np.random.rand(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "        'W2':np.random.rand(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "        'W3':np.random.rand(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "    }\n",
        "\n",
        "    return params\n",
        "\n",
        "  def forward_pass(self, x_train):\n",
        "    params = self.params\n",
        "\n",
        "    params['A0'] = x_train\n",
        "\n",
        "    params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "    params['A1'] = self.ReLu(params['Z1'])\n",
        "\n",
        "    params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "    params['A2'] = self.ReLu(params['Z2'])\n",
        "\n",
        "    params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "    params['A3'] = self.sigmoid(params['Z3'])\n",
        "\n",
        "    return params['A3']\n",
        "\n",
        "  def backward_pass(self, y_train, output):\n",
        "    params = self.params\n",
        "    change_w = {}\n",
        "\n",
        "    # Calculate W3 update\n",
        "    error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], der=True)\n",
        "    change_w['W3'] = np.outer(error, params['A2'])\n",
        "\n",
        "    # Calculate W2 update\n",
        "    error = np.dot(params['W3'].T, error) * self.ReLu(params['Z2'], der=True)\n",
        "    change_w['W2'] = np.outer(error, params['A1'])\n",
        "\n",
        "    # Calculate W1 update\n",
        "    error = np.dot(params['W2'].T, error) * self.ReLu(params['Z1'], der=True)\n",
        "    change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "    return change_w\n",
        "\n",
        "  def update_network_parameters(self, changes_to_w):\n",
        "    for key, value in changes_to_w.items():\n",
        "            self.params[key] -= self.l_rate * value\n",
        "  \n",
        "  def compute_accuracy(self, x_val, y_val):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_val, y_val):\n",
        "      output = self.forward_pass(x)\n",
        "      pred = np.argmax(output)\n",
        "      predictions.append(pred == np.argmax(y))\n",
        "    \n",
        "    return np.mean(predictions)\n",
        "  \n",
        "  def train(self, x_train, y_train, x_val, y_val):\n",
        "    start_time = time.time()\n",
        "    for iteration in range(self.epochs):\n",
        "      for x, y in zip(x_train, y_train):\n",
        "        output = self.forward_pass(x)\n",
        "        changes_to_w = self.backward_pass(y, output)\n",
        "        self.update_network_parameters(changes_to_w)\n",
        "      \n",
        "      accuracy = self.compute_accuracy(x_val, y_val)\n",
        "      print('Epoch: {0}, Time spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "          iteration+1, time.time() - start_time, accuracy * 100\n",
        "      ))\n",
        "    \n",
        "  def predict(self, x_test):\n",
        "    predictions = []\n",
        "    for x in (x_test):\n",
        "      output = self.forward_pass(x)\n",
        "      predictions.append(np.argmax(output))\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBfYe4ROog_I"
      },
      "source": [
        "**Training . . .**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yicjAO7ogby",
        "outputId": "d4425b0d-8d55-4963-a29c-cfea851e93bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time spent: 1.06s, Accuracy: 35.17%\n",
            "Epoch: 2, Time spent: 2.14s, Accuracy: 72.42%\n",
            "Epoch: 3, Time spent: 3.20s, Accuracy: 66.12%\n",
            "Epoch: 4, Time spent: 4.26s, Accuracy: 47.54%\n",
            "Epoch: 5, Time spent: 5.30s, Accuracy: 48.83%\n",
            "Epoch: 6, Time spent: 6.34s, Accuracy: 47.58%\n",
            "Epoch: 7, Time spent: 7.49s, Accuracy: 48.38%\n",
            "Epoch: 8, Time spent: 8.53s, Accuracy: 49.50%\n",
            "Epoch: 9, Time spent: 9.57s, Accuracy: 50.25%\n",
            "Epoch: 10, Time spent: 10.62s, Accuracy: 50.71%\n",
            "Epoch: 11, Time spent: 11.68s, Accuracy: 57.00%\n",
            "Epoch: 12, Time spent: 12.74s, Accuracy: 56.46%\n",
            "Epoch: 13, Time spent: 13.82s, Accuracy: 55.21%\n",
            "Epoch: 14, Time spent: 14.95s, Accuracy: 54.75%\n",
            "Epoch: 15, Time spent: 16.01s, Accuracy: 53.67%\n",
            "Epoch: 16, Time spent: 17.09s, Accuracy: 53.96%\n",
            "Epoch: 17, Time spent: 18.16s, Accuracy: 53.33%\n",
            "Epoch: 18, Time spent: 19.21s, Accuracy: 51.54%\n",
            "Epoch: 19, Time spent: 20.31s, Accuracy: 51.21%\n",
            "Epoch: 20, Time spent: 21.34s, Accuracy: 50.33%\n",
            "Epoch: 21, Time spent: 22.40s, Accuracy: 50.33%\n",
            "Epoch: 22, Time spent: 23.44s, Accuracy: 49.75%\n",
            "Epoch: 23, Time spent: 24.48s, Accuracy: 50.29%\n",
            "Epoch: 24, Time spent: 25.51s, Accuracy: 48.29%\n",
            "Epoch: 25, Time spent: 26.55s, Accuracy: 50.00%\n",
            "Epoch: 26, Time spent: 27.62s, Accuracy: 48.12%\n",
            "Epoch: 27, Time spent: 28.67s, Accuracy: 48.67%\n",
            "Epoch: 28, Time spent: 29.72s, Accuracy: 48.75%\n",
            "Epoch: 29, Time spent: 30.76s, Accuracy: 49.08%\n",
            "Epoch: 30, Time spent: 31.80s, Accuracy: 48.17%\n",
            "Epoch: 31, Time spent: 32.85s, Accuracy: 48.88%\n",
            "Epoch: 32, Time spent: 33.89s, Accuracy: 48.29%\n",
            "Epoch: 33, Time spent: 34.93s, Accuracy: 48.88%\n",
            "Epoch: 34, Time spent: 35.98s, Accuracy: 49.67%\n",
            "Epoch: 35, Time spent: 37.03s, Accuracy: 50.92%\n",
            "Epoch: 36, Time spent: 38.13s, Accuracy: 51.08%\n",
            "Epoch: 37, Time spent: 39.19s, Accuracy: 51.42%\n",
            "Epoch: 38, Time spent: 40.23s, Accuracy: 51.71%\n",
            "Epoch: 39, Time spent: 41.28s, Accuracy: 51.92%\n",
            "Epoch: 40, Time spent: 42.32s, Accuracy: 52.04%\n",
            "Epoch: 41, Time spent: 43.38s, Accuracy: 52.04%\n",
            "Epoch: 42, Time spent: 44.42s, Accuracy: 51.88%\n",
            "Epoch: 43, Time spent: 45.48s, Accuracy: 51.67%\n",
            "Epoch: 44, Time spent: 46.52s, Accuracy: 51.38%\n",
            "Epoch: 45, Time spent: 47.57s, Accuracy: 51.17%\n",
            "Epoch: 46, Time spent: 48.63s, Accuracy: 51.38%\n",
            "Epoch: 47, Time spent: 49.67s, Accuracy: 51.08%\n",
            "Epoch: 48, Time spent: 50.71s, Accuracy: 52.04%\n",
            "Epoch: 49, Time spent: 51.75s, Accuracy: 51.62%\n",
            "Epoch: 50, Time spent: 52.79s, Accuracy: 51.62%\n",
            "Epoch: 51, Time spent: 53.85s, Accuracy: 51.71%\n",
            "Epoch: 52, Time spent: 54.90s, Accuracy: 51.50%\n",
            "Epoch: 53, Time spent: 55.96s, Accuracy: 51.88%\n",
            "Epoch: 54, Time spent: 57.00s, Accuracy: 51.88%\n",
            "Epoch: 55, Time spent: 58.04s, Accuracy: 51.75%\n",
            "Epoch: 56, Time spent: 59.08s, Accuracy: 51.67%\n",
            "Epoch: 57, Time spent: 60.13s, Accuracy: 51.46%\n",
            "Epoch: 58, Time spent: 61.16s, Accuracy: 51.58%\n",
            "Epoch: 59, Time spent: 62.20s, Accuracy: 51.62%\n",
            "Epoch: 60, Time spent: 63.25s, Accuracy: 52.21%\n",
            "Epoch: 61, Time spent: 64.29s, Accuracy: 52.04%\n",
            "Epoch: 62, Time spent: 65.33s, Accuracy: 52.29%\n",
            "Epoch: 63, Time spent: 66.39s, Accuracy: 52.38%\n",
            "Epoch: 64, Time spent: 67.42s, Accuracy: 52.46%\n",
            "Epoch: 65, Time spent: 68.49s, Accuracy: 52.46%\n",
            "Epoch: 66, Time spent: 69.52s, Accuracy: 52.42%\n",
            "Epoch: 67, Time spent: 70.56s, Accuracy: 52.75%\n",
            "Epoch: 68, Time spent: 71.64s, Accuracy: 52.42%\n",
            "Epoch: 69, Time spent: 72.70s, Accuracy: 52.96%\n",
            "Epoch: 70, Time spent: 73.81s, Accuracy: 52.83%\n",
            "Epoch: 71, Time spent: 74.86s, Accuracy: 52.79%\n",
            "Epoch: 72, Time spent: 75.91s, Accuracy: 53.08%\n",
            "Epoch: 73, Time spent: 76.95s, Accuracy: 53.25%\n",
            "Epoch: 74, Time spent: 78.00s, Accuracy: 53.29%\n",
            "Epoch: 75, Time spent: 79.04s, Accuracy: 53.21%\n",
            "Epoch: 76, Time spent: 80.08s, Accuracy: 53.25%\n",
            "Epoch: 77, Time spent: 81.15s, Accuracy: 53.33%\n",
            "Epoch: 78, Time spent: 82.19s, Accuracy: 53.58%\n",
            "Epoch: 79, Time spent: 83.88s, Accuracy: 53.58%\n",
            "Epoch: 80, Time spent: 85.53s, Accuracy: 53.29%\n",
            "Epoch: 81, Time spent: 86.57s, Accuracy: 53.29%\n",
            "Epoch: 82, Time spent: 88.00s, Accuracy: 53.29%\n",
            "Epoch: 83, Time spent: 90.08s, Accuracy: 53.42%\n",
            "Epoch: 84, Time spent: 91.56s, Accuracy: 53.29%\n",
            "Epoch: 85, Time spent: 93.18s, Accuracy: 53.29%\n",
            "Epoch: 86, Time spent: 94.41s, Accuracy: 53.58%\n",
            "Epoch: 87, Time spent: 95.45s, Accuracy: 53.50%\n",
            "Epoch: 88, Time spent: 96.50s, Accuracy: 53.58%\n",
            "Epoch: 89, Time spent: 97.96s, Accuracy: 53.46%\n",
            "Epoch: 90, Time spent: 99.47s, Accuracy: 53.50%\n",
            "Epoch: 91, Time spent: 100.89s, Accuracy: 53.50%\n",
            "Epoch: 92, Time spent: 102.28s, Accuracy: 53.50%\n",
            "Epoch: 93, Time spent: 103.87s, Accuracy: 53.50%\n",
            "Epoch: 94, Time spent: 105.51s, Accuracy: 53.62%\n",
            "Epoch: 95, Time spent: 107.04s, Accuracy: 53.92%\n",
            "Epoch: 96, Time spent: 108.46s, Accuracy: 53.71%\n",
            "Epoch: 97, Time spent: 109.91s, Accuracy: 53.71%\n",
            "Epoch: 98, Time spent: 111.97s, Accuracy: 53.79%\n",
            "Epoch: 99, Time spent: 114.09s, Accuracy: 53.83%\n",
            "Epoch: 100, Time spent: 115.75s, Accuracy: 53.75%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "mlp = MultiLayerPerceptron(sizes=[301, 40, 10, 2], epochs = 100, l_rate=0.005)\n",
        "mlp.train(X_train, Y_train, X_val, Y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict and compute accuracy:**"
      ],
      "metadata": {
        "id": "F4JK7n8FdXNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict = mlp.predict(test[:10])\n",
        "output_dict = {\"Word1\" : p_word1[:10],\n",
        "               \"Word2\" : p_word2[:10],\n",
        "               \"Cosine Similarity\": css[:10],\n",
        "               \"Predicted\": predict,\n",
        "               \"True Value\": type_of_word[:10]\n",
        "               }\n",
        "out_test = pd.DataFrame(output_dict)\n",
        "print(metrics.classification_report(out_test['True Value'], predict))\n",
        "print(\"____________________________________________________________________\")\n",
        "print(\"1 is Synonym and 0 is Antonym, Output test Visualize:\")\n",
        "print(\"____________________________________________________________________\")\n",
        "print(out_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B3uDyetdWy3",
        "outputId": "a54b0eb8-975a-4c8f-986f-62254aaebda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.50      0.57         8\n",
            "           1       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.40        10\n",
            "   macro avg       0.33      0.25      0.29        10\n",
            "weighted avg       0.53      0.40      0.46        10\n",
            "\n",
            "____________________________________________________________________\n",
            "1 is Synonym and 0 is Antonym, Output test Visualize:\n",
            "____________________________________________________________________\n",
            "          Word1      Word2  Cosine Similarity  Predicted  True Value\n",
            "0     khoái_lạc    nỗi_đau           0.622883          0           0\n",
            "1       yếu_kém   sức_mạnh           0.585686          0           0\n",
            "2    thanh_danh     ô_nhục           0.544493          0           0\n",
            "3           dây        dọc           0.570831          0           1\n",
            "4            ba         me           0.475859          1           0\n",
            "5          ngày    ban_đêm           0.576550          1           0\n",
            "6      nhân_tài   tài_năng           0.769763          0           1\n",
            "7      khởi_đầu  cuối_cùng           0.573943          0           0\n",
            "8       hậu_duệ      bố_mẹ           0.516518          1           0\n",
            "9  khuyết_thiếu    ưu_điểm           0.556300          1           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result is very bad!!! =>>>>> Underfitting: Model Cannot Fitting**"
      ],
      "metadata": {
        "id": "wUjBUlqYfHPo"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}