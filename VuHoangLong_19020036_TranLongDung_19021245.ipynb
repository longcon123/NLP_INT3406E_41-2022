{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocmcqtWyJEch"
      },
      "source": [
        "# NLP: INT3406E_41 (2022) - PGS.TS: Nguyen Phuong Thai\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "PROGRAMMING ASSIGNMENT 1: WORD SIMILARITY AND SEMANTIC RELATION CLASSIFICATION\n",
        "#Vũ Hoàng Long 19020036\n",
        "#Trần Long Dũng 19021245"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKHBBzOnEItQ",
        "outputId": "80a2f999-727a-4a5b-af5e-05823a37eb9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Cloning into 'Word-Similarity'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Total 196 (delta 0), reused 0 (delta 0), pack-reused 196\u001b[K\n",
            "Receiving objects: 100% (196/196), 5.81 MiB | 28.74 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!git clone https://github.com/NLP-Projects/Word-Similarity.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIV25hQsFwGo"
      },
      "source": [
        "# EX1: Cosine similarity:\n",
        "Given pre-trained embeddings of Vietnamese words, implement a function for calculating cosine similarity between word pairs. Test your program using word pairs in ViSim-400 dataset (in directory Datasets/ViSim-400)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD1zHlUcHaRT"
      },
      "source": [
        "**Dowload pretrained Word2vec Dataset:**\n",
        "Link download pre-trained word2vec model \n",
        "https://drive.google.com/open?id=1z1IDKaZuJXw5g7Yebr1GDq2UfvVR_aGx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SVrwxLFFiUt",
        "outputId": "1e28dfa4-7428-4ea7-a76d-ac8f8b093e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z1IDKaZuJXw5g7Yebr1GDq2UfvVR_aGx\n",
            "To: /content/W2V_150.txt\n",
            "100% 119M/119M [00:00<00:00, 159MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1z1IDKaZuJXw5g7Yebr1GDq2UfvVR_aGx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoK3yqEuUf3w"
      },
      "source": [
        "**Extract pretrained word2vec data into a Dictionary for searching embedded vectors by key work**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0N8HQJoWaEn0"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "vec_data_file = '/content/W2V_150.txt' #define path of data file\n",
        "def word2vec_dictionary(data_file):\n",
        "  emb_vec_dict = {}\n",
        "  with open(data_file, 'r') as fd: #read data\n",
        "      reader = csv.reader(fd)\n",
        "      for row in reader:\n",
        "        split_data = row[0].split(' ') #split data by column\n",
        "        word = split_data[0]\n",
        "        emb_vec = [] #create new list for containing all the 150 vectors\n",
        "        for v in split_data[2:-1]: #split 150 column of vectors\n",
        "          emb_vec.append(float(v)) #parsing the string vector to float number and add to list\n",
        "        emb_vec_dict[word] = emb_vec #add key word and value of 150 vectors of that word into dictionary\n",
        "  return emb_vec_dict\n",
        "word2vec = word2vec_dictionary(vec_data_file) #create new dictionary object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uooL5XGD5mon",
        "outputId": "d2d8713f-da5b-41f0-ea6f-1f5225da803b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9002099, 0.4414572, 0.3195622, 0.1525354, -0.8680423, -0.9192941, 1.724312, 0.129385, 0.5892586, -0.9284741, -1.090982, 0.3210891, -0.05774014, 0.5338939, -1.180989, 0.5865031, -0.632705, -1.133461, -0.07345625, 0.7923573, -0.1071931, 0.06241727, -0.03073531, 0.2454646, -0.1052362, -3.101819, 1.593211, -1.690924, -1.489635, 1.260306, -0.2791042, -1.138475, -2.131525, -0.6840508, -1.942854, -0.4895685, 1.624937, 0.5780962, 0.02403186, -0.08410149, -0.9141436, -1.183267, -0.3039933, -1.103466, -1.340766, 0.147565, 0.7379212, 0.7285, -1.895763, 1.505164, -1.396337, 0.4565657, -0.3163474, 0.1690939, 3.292289, 0.02980663, -0.3962629, -0.6508451, 0.1695397, 3.389216, -1.18901, 1.98491, 0.5849065, -0.6100891, 0.1037576, 0.6502007, 0.2622414, -0.6728628, 0.5830782, -0.2222768, 0.8332402, -0.8051164, 1.258237, -1.051222, -1.486286, -0.007887463, -0.8877624, 1.526078, -0.6547452, 1.070157, 1.614038, 0.89986, -2.102119, 0.769101, -1.317077, 0.3960995, 0.7461163, 0.3436002, -2.885336, -0.1215674, 0.1706368, -2.590349, 1.116513, -2.55688, -0.1204964, 2.050935, 1.759924, 1.488335, 2.209169, -1.029199, -1.186378, -1.561149, -0.6596498, -2.658225, 0.2231091, -0.4475015, 0.06487254, 2.473543, -0.4470818, -0.8897195, 1.126703, -0.6788828, -0.2254429, 2.662564, 0.01999, -0.103356, -0.5815216, 0.577478, -0.8818318, 0.9079844, -1.329538, -0.5213899, 0.03550991, 0.7110145, 0.460395, 0.5177755, 0.400955, 1.963867, 2.062421, -1.20937, 0.5540814, -0.4141474, 2.371543, -1.97647, 1.277484, 0.05288968, 0.2223771, 0.4217772, -0.07227067, -0.3532057, 1.264529, 0.3869175, -1.800506, 0.1288714, 0.2647036, -1.256201, 0.4180586, 0.7004945, -0.2989486, -1.471938]\n",
            "150\n"
          ]
        }
      ],
      "source": [
        "word = 'đẹp_trai'\n",
        "print(word2vec[word])\n",
        "print(len(word2vec[word]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-2L9AZpKOTg"
      },
      "source": [
        "**Define the cosine_similarity function for calculating the cosine similarity between 2 vecs:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzpVumVwFi9C",
        "outputId": "2614d6bc-0189-4f6a-a558-9e182673f59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9844319658134832\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import spatial\n",
        "\n",
        "a = [1,2,3,4]\n",
        "b = [5,6,7,8]\n",
        "def cosine_similarity_cal(v1, v2):\n",
        "  return (2 - spatial.distance.cosine(v1, v2))/2\n",
        "print(cosine_similarity_cal(a,b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po3n8aS9lw0E"
      },
      "source": [
        "**Calculate the cosine similarity between 2 word by vec:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yiqucz42Eld1",
        "outputId": "b1092723-ac14-4cf2-9307-605e0a23e7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This compair word was not found in dictionary!\n",
            "Pair-Word: biến - ngập, Cosine-similarity: 0.497543830265165\n",
            "Pair-Word: nhà_thi_đấu - nhà, Cosine-similarity: 0.5412615916460588\n",
            "Pair-Word: động - tĩnh, Cosine-similarity: 0.6385429799341388\n",
            "Pair-Word: khuyết - ưu, Cosine-similarity: 0.5883993141781336\n",
            "This compair word was not found in dictionary!\n",
            "Pair-Word: thủ_pháp - biện_pháp, Cosine-similarity: 0.7011830645971531\n",
            "Pair-Word: kết_duyên - thành_hôn, Cosine-similarity: 0.7315042010070361\n",
            "Pair-Word: cấp_tiến - bảo_thủ, Cosine-similarity: 0.6284735044498062\n",
            "Pair-Word: nước_lớn - nguy_hiểm, Cosine-similarity: 0.5925960112010581\n",
            "Pair-Word: hoa - nhị, Cosine-similarity: 0.6115348022627427\n",
            "Pair-Word: bất_lợi - thuận_lợi, Cosine-similarity: 0.7674456514981696\n",
            "Pair-Word: phân_ly - sum_họp, Cosine-similarity: 0.5388505446519811\n",
            "This compair word was not found in dictionary!\n",
            "This compair word was not found in dictionary!\n",
            "Pair-Word: diễu_hành - tuần_hành, Cosine-similarity: 0.8219417700919267\n",
            "Pair-Word: cao_trào - thoái_trào, Cosine-similarity: 0.6805162763864085\n",
            "Pair-Word: thịnh - suy, Cosine-similarity: 0.5298011854246096\n",
            "Pair-Word: con_voi - ngà, Cosine-similarity: 0.6194775176849935\n",
            "Pair-Word: cá_chuối - cá_quả, Cosine-similarity: 0.5832370695737137\n",
            "Pair-Word: đổ_bê_tông - biếu_xén, Cosine-similarity: 0.41357996395190266\n",
            "Pair-Word: con_đẻ - con_ruột, Cosine-similarity: 0.8208944017358941\n",
            "Pair-Word: uỷ_nhiệm - phân_công, Cosine-similarity: 0.6437676389552878\n",
            "Pair-Word: bay_lượn - chuyển_động, Cosine-similarity: 0.6940985847220473\n",
            "Pair-Word: khêu_gợi - khơi_gợi, Cosine-similarity: 0.6279823643980049\n",
            "Pair-Word: thăng_bằng - cân_bằng, Cosine-similarity: 0.7402159711927809\n",
            "Pair-Word: chọn_lựa - tuyển_chọn, Cosine-similarity: 0.7362131494371043\n",
            "Pair-Word: có_ích - hữu_ích, Cosine-similarity: 0.831204692617032\n",
            "Pair-Word: cứng_rắn - nhỏ_nhắn, Cosine-similarity: 0.61261657499084\n",
            "This compair word was not found in dictionary!\n",
            "Pair-Word: lỗ_đen - hố_đen, Cosine-similarity: 0.8670341671180348\n",
            "Pair-Word: vùi - đóng, Cosine-similarity: 0.49319522774025804\n",
            "Pair-Word: tính_tình - tính_khí, Cosine-similarity: 0.8439127210129063\n",
            "Pair-Word: mục_sư - giáo_sĩ, Cosine-similarity: 0.7982022218167744\n",
            "Pair-Word: cộng_đồng - dân_tộc, Cosine-similarity: 0.6877220711062354\n",
            "Pair-Word: rớ - sờ, Cosine-similarity: 0.6928067911025224\n",
            "Pair-Word: trả_đũa - đền_đáp, Cosine-similarity: 0.6165282320855275\n",
            "Pair-Word: tay_cầm - phân_đạm, Cosine-similarity: 0.48487143158068235\n",
            "Pair-Word: liên_hệ - nguỵ_trang, Cosine-similarity: 0.5461990752666104\n",
            "Pair-Word: mới_tinh - mới_toanh, Cosine-similarity: 0.7406450974879364\n",
            "Pair-Word: cột_trụ - bệ, Cosine-similarity: 0.7215144241298665\n",
            "Pair-Word: họp_mặt - tiệc, Cosine-similarity: 0.7138201028107132\n",
            "Pair-Word: lộn_xộn - gọn_gàng, Cosine-similarity: 0.6717686073142879\n",
            "Pair-Word: chủ_động - thụ_động, Cosine-similarity: 0.7388768020216623\n",
            "Pair-Word: vali - tay_cầm, Cosine-similarity: 0.582991276995551\n",
            "This compair word was not found in dictionary!\n",
            "Pair-Word: ghen_tị - ghen_ghét, Cosine-similarity: 0.675940010814073\n",
            "This compair word was not found in dictionary!\n",
            "Pair-Word: ngôn_ngữ_học - khoa_học, Cosine-similarity: 0.7686254254730083\n",
            "This compair word was not found in dictionary!\n"
          ]
        }
      ],
      "source": [
        "data_file = '/content/Word-Similarity/datasets/ViSim-400/Visim-400.txt'\n",
        "#data = f = open(data_file, \"r\")\n",
        "import csv\n",
        "i = 0\n",
        "n_compair = 50\n",
        "with open(data_file, 'r') as fd:\n",
        "    reader = csv.reader(fd)\n",
        "    for row in reader:\n",
        "      split_data = row[0].split('\\t')\n",
        "      word1 = split_data[0]\n",
        "      word2 = split_data[1]\n",
        "      try:\n",
        "        vec1 = np.array(word2vec[word1])\n",
        "        vec2 = np.array(word2vec[word2])\n",
        "        print(\"Pair-Word: {} - {}, Cosine-similarity: {}\".format(word1, word2, cosine_similarity_cal(vec1, vec2)))\n",
        "      except:\n",
        "        print('This compair word was not found in dictionary!')\n",
        "      i+=1\n",
        "      if i == n_compair:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XBBHMWyyXLp"
      },
      "source": [
        "# EX2: K-nearest words: Given a word w, find k most-similar words of w using the function implemented in question 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVWu9VEP_d4q"
      },
      "source": [
        "**Find k nearest text function:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYe3sjGgyg4K",
        "outputId": "3df952ce-4734-442c-8999-4e78c6d7db46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input w word: báo_đốm\n",
            "   nước_lớn  nguy_hiểm      xiếc      ngựa       cá  móng_vuốt     sư_tử  \\\n",
            "0  0.490419   0.515939  0.589138  0.618687  0.65813   0.667844  0.685771   \n",
            "\n",
            "         hổ      mèo   bộ_lông  \n",
            "0  0.715635  0.72033  0.741457  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "def k_nearest_neighbor(w, v, k = 1):\n",
        "  similar_vec = [] #create a similar vec list for choosing a nearest vector\n",
        "  map_v2w = {} #create a dictionary for mapping the vector to word after find the k nearest\n",
        "  output = {}\n",
        "  for i in range(len(v)):\n",
        "    w_vec = word2vec[w] #search the embedding vector of the word\n",
        "    word_vec = word2vec[v[i]] #search the embedding vector of the word\n",
        "    csc = cosine_similarity_cal(w_vec, word_vec) #calculate the cosine similarity\n",
        "    similar_vec.append(csc) #append similar score to list for searching the nearest\n",
        "    map_v2w[csc] = v[i] #adding the key: vector and value: word to dictionary for quering the word which is nearest\n",
        "  similar_vec.sort() #sorting by ascending order\n",
        "  k_nearest_vec = similar_vec[-k:] #quering the k-nearest vector\n",
        "  for vec in k_nearest_vec:\n",
        "    word = map_v2w[vec] #mapping the k-nearest vector to word\n",
        "    output[word] = vec\n",
        "  return output\n",
        "#Testing function\n",
        "k = 10\n",
        "w = \"báo_đốm\"\n",
        "v = [\"nước_lớn\", \"tiền\", \"nhà_thi_đấu\", \"kết_duyên\", \"mèo\", \"xiếc\", \"nguy_hiểm\", \"sóng\", \n",
        "     \"hổ\", \"sư_tử\", \"móng_vuốt\",\"bộ_lông\", \"cá\", \"ngựa\", \"nhà_nước\"]\n",
        "print(\"Input w word: {}\".format(w))\n",
        "df = pd.DataFrame(k_nearest_neighbor(w, v, k), index=[0])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNDqw0i9YopJ"
      },
      "source": [
        "# EX3: Synonym-antonym classification: Implement a Logistic Regression classifier or a Multi-layer Perceptron classifier for distinguishing synonyms and antonyms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfEX4eR7vIhR"
      },
      "source": [
        "**Extract 301 Vector: (150 each word) and Sim Score (1 each pair-word) of Synonym and Antonym dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8aEXhwRdvITd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "ant_file = '/content/Word-Similarity/antonym-synonym set/Antonym_vietnamese.txt'\n",
        "syn_file = '/content/Word-Similarity/antonym-synonym set/Synonym_vietnamese.txt'\n",
        "\n",
        "css = []\n",
        "type_of_word = []\n",
        "dict_dataset = {}\n",
        "dict_lists = {}\n",
        "for i in range(1,301):\n",
        "  dict_lists['l'+str(i)] = []\n",
        "with open(ant_file, 'r') as fd:\n",
        "    for row in fd: \n",
        "      split_word = row.split(' ')\n",
        "      try: #try if dictionary have that word\n",
        "        word1 = split_word[0].strip()\n",
        "        word2 = split_word[1].strip()\n",
        "        vec1 = np.array(word2vec[word1])\n",
        "        vec2 = np.array(word2vec[word2])\n",
        "        vec12 = np.concatenate((vec1, vec2))\n",
        "        css.append(cosine_similarity_cal(vec1, vec2))\n",
        "        type_of_word.append(0)\n",
        "        for i in range(300):\n",
        "            dict_lists['l'+str(i+1)].append(vec12[i])\n",
        "      except:\n",
        "        pass\n",
        "with open(syn_file, 'r') as fd:\n",
        "    for row in fd:\n",
        "      split_word = row.split(' ')\n",
        "      try: #try if dictionary have that word\n",
        "        word1 = split_word[0].strip()\n",
        "        word2 = split_word[1].strip()\n",
        "        vec1 = word2vec[word1]\n",
        "        vec2 = word2vec[word2]\n",
        "        vec12 = np.concatenate((vec1, vec2))\n",
        "        css.append(cosine_similarity_cal(vec1, vec2))\n",
        "        type_of_word.append(1)\n",
        "        for i in range(300):\n",
        "            dict_lists['l'+str(i+1)].append(vec12[i])\n",
        "      except:\n",
        "        pass\n",
        "dict_dataset['CSS'] = css\n",
        "dict_dataset['Type'] = type_of_word\n",
        "for i in range(300):\n",
        "  dict_dataset['vec'+str(i+1)] = dict_lists['l'+str(i+1)]\n",
        "df = pd.DataFrame(dict_dataset)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "df_no_type = df.drop('Type', axis=1)\n",
        "features = df_no_type.to_numpy()\n",
        "labels = df['Type'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fSzt0yA2ub3",
        "outputId": "a291acb8-c0f4-49b8-e510-4ecdaf3e24e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           CSS  Type      vec1      vec2      vec3      vec4      vec5  \\\n",
            "4450  0.753108     1 -0.374032  4.361281 -0.453997  0.749064  1.222181   \n",
            "2971  0.559782     1  1.536286  0.698250 -1.203994  1.087432  1.033297   \n",
            "4165  0.561599     1  0.217570 -1.699579  1.219833 -0.639883  2.039269   \n",
            "7357  0.616358     1  0.089850 -0.842885 -1.158304 -1.052221 -0.185887   \n",
            "7661  0.898512     1  0.334460  0.180449  0.065709 -0.751046 -0.822057   \n",
            "...        ...   ...       ...       ...       ...       ...       ...   \n",
            "7826  0.836643     1  0.367634 -2.305648 -0.385922 -1.149660  0.859798   \n",
            "2332  0.680168     1 -0.216281  1.090705 -1.500688  1.100905  0.798344   \n",
            "1658  0.612497     0  0.161608  0.995400 -0.277558  1.630965  2.345793   \n",
            "1967  0.562264     1  0.907201  0.761925 -0.986238  1.355984  0.025384   \n",
            "3900  0.536655     1 -0.725507  0.873804  3.612312  0.289523  0.662060   \n",
            "\n",
            "          vec6      vec7      vec8  ...    vec291    vec292    vec293  \\\n",
            "4450  1.807540 -0.758798  1.175439  ... -0.190532  2.244146 -1.063388   \n",
            "2971 -0.313630  0.890501 -0.717488  ... -1.323915 -0.085220  1.325930   \n",
            "4165 -1.486156  0.758957 -0.921713  ...  0.095581  0.863128 -1.365879   \n",
            "7357  1.070050 -0.670242 -0.570623  ... -0.707325  1.595374 -0.481793   \n",
            "7661  0.895872 -0.727170  0.498009  ...  0.538559 -0.660306  0.636031   \n",
            "...        ...       ...       ...  ...       ...       ...       ...   \n",
            "7826 -1.256389  0.156378  0.880283  ...  0.813520  1.744824 -0.523346   \n",
            "2332 -0.140205  0.775595 -0.677123  ... -0.306999  0.317478  1.057265   \n",
            "1658  0.425154  0.478258  0.969055  ...  1.337098 -0.207455  0.115575   \n",
            "1967  2.617326 -0.017823 -0.195483  ...  0.676320  1.045042 -0.499895   \n",
            "3900  1.282580  0.293764  1.302423  ... -0.036593 -0.754388  0.427379   \n",
            "\n",
            "        vec294    vec295    vec296    vec297    vec298    vec299    vec300  \n",
            "4450  1.519379 -0.886589 -2.625728 -0.468153  0.047883 -1.432239 -3.128659  \n",
            "2971 -0.539056 -1.010415  1.395917 -1.220106  0.804552  0.774003  0.388367  \n",
            "4165  0.887375 -1.301042 -0.243695 -1.193012 -1.916067  0.762816 -0.670064  \n",
            "7357  0.903668 -0.900280 -0.017886 -1.072671 -0.054945 -1.016567 -0.309416  \n",
            "7661  1.167283 -1.197553  0.394202  0.473076 -0.865615 -0.774344  0.456853  \n",
            "...        ...       ...       ...       ...       ...       ...       ...  \n",
            "7826 -0.908172  1.097503  0.362985 -1.304803 -0.777923  0.974640 -2.076595  \n",
            "2332 -0.129335 -0.912381  1.219113 -0.437022  0.027127 -0.952981  1.693631  \n",
            "1658 -0.367094 -0.809489 -1.103836 -0.824847  0.573913 -0.483240 -0.323133  \n",
            "1967 -0.068218 -0.264290 -0.551972 -4.042773  1.282595 -0.503265 -0.037695  \n",
            "3900 -0.601407  1.327125 -1.526804  2.025993 -0.018861  1.071217 -0.138422  \n",
            "\n",
            "[7998 rows x 302 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KR_Zw0IepJf3"
      },
      "outputs": [],
      "source": [
        "df.to_csv('Word_pair_by_Vec_Data.csv', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NXLa2Jx21YQ"
      },
      "source": [
        "**Split data for training and testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfxyR6fNvqE7",
        "outputId": "9eceac60-24f2-4bc7-87d2-90064ca89d7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training records: 5598\n",
            "Val records: 2400\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(features, labels, test_size=0.30)\n",
        "print('Training records:', Y_train.size)\n",
        "print('Val records:', Y_val.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing training with sklearn nn MLP classifier**"
      ],
      "metadata": {
        "id": "rGBQRzdYNr89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6VMZ7e65EKG",
        "outputId": "64c6c146-d3f5-49c4-c3bf-ab5b0c43ef1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.40042772\n",
            "Iteration 2, loss = 0.20636114\n",
            "Iteration 3, loss = 0.10344609\n",
            "Iteration 4, loss = 0.04724831\n",
            "Iteration 5, loss = 0.02496205\n",
            "Iteration 6, loss = 0.02226124\n",
            "Iteration 7, loss = 0.02720229\n",
            "Iteration 8, loss = 0.02761169\n",
            "Iteration 9, loss = 0.02078475\n",
            "Iteration 10, loss = 0.01151792\n",
            "Iteration 11, loss = 0.00948655\n",
            "Iteration 12, loss = 0.00975757\n",
            "Iteration 13, loss = 0.00843164\n",
            "Iteration 14, loss = 0.00741934\n",
            "Iteration 15, loss = 0.00523630\n",
            "Iteration 16, loss = 0.00403942\n",
            "Iteration 17, loss = 0.00144284\n",
            "Iteration 18, loss = 0.00509128\n",
            "Iteration 19, loss = 0.00178694\n",
            "Iteration 20, loss = 0.00370857\n",
            "Iteration 21, loss = 0.00081636\n",
            "Iteration 22, loss = 0.00081327\n",
            "Iteration 23, loss = 0.00070201\n",
            "Iteration 24, loss = 0.00060100\n",
            "Iteration 25, loss = 0.00073697\n",
            "Iteration 26, loss = 0.00063680\n",
            "Iteration 27, loss = 0.00068360\n",
            "Iteration 28, loss = 0.00092616\n",
            "Iteration 29, loss = 0.00137698\n",
            "Iteration 30, loss = 0.00235290\n",
            "Iteration 31, loss = 0.00098972\n",
            "Iteration 32, loss = 0.00103246\n",
            "Iteration 33, loss = 0.00087671\n",
            "Iteration 34, loss = 0.00086902\n",
            "Iteration 35, loss = 0.00089564\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(64, 32), learning_rate_init=0.01,\n",
              "              random_state=1, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Import MLPClassifer \n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create model object\n",
        "clf = MLPClassifier(hidden_layer_sizes=(64,32),\n",
        "                    random_state=1,\n",
        "                    verbose=True,\n",
        "                    learning_rate_init=0.01)\n",
        "\n",
        "# Fit data onto the model\n",
        "clf.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST THE MODEL WITH VISIM400 and print OUTPUT**"
      ],
      "metadata": {
        "id": "gIGkVNZiG73Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucILe4cozwkO",
        "outputId": "897d9c76-6101-4fe0-ab8a-69d7743d016d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:\n",
            "___________________________________________________________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        32\n",
            "           1       0.95      1.00      0.97        18\n",
            "\n",
            "    accuracy                           0.98        50\n",
            "   macro avg       0.97      0.98      0.98        50\n",
            "weighted avg       0.98      0.98      0.98        50\n",
            "\n",
            "____________________________________________________________________\n",
            "1 is Synonym and 0 is Antonym, Output test Visualize:\n",
            "____________________________________________________________________\n",
            "           Word1        Word2  Cosine Similarity  Predicted  True Value\n",
            "0      khoái_lạc      nỗi_đau           0.622883          0           0\n",
            "1        yếu_kém     sức_mạnh           0.585686          0           0\n",
            "2     thanh_danh       ô_nhục           0.544493          0           0\n",
            "3            dây          dọc           0.570831          1           1\n",
            "4             ba           me           0.475859          0           0\n",
            "5           ngày      ban_đêm           0.576550          0           0\n",
            "6       nhân_tài     tài_năng           0.769763          1           1\n",
            "7       khởi_đầu    cuối_cùng           0.573943          0           0\n",
            "8        hậu_duệ        bố_mẹ           0.516518          0           0\n",
            "9   khuyết_thiếu      ưu_điểm           0.556300          0           0\n",
            "10       áo_quan     quan_tài           0.837791          1           1\n",
            "11     phục_viên    động_viên           0.582829          0           0\n",
            "12           tía           me           0.643729          0           0\n",
            "13           cậu           cô           0.845036          1           0\n",
            "14     xuất_phát     kết_thúc           0.536195          0           0\n",
            "15       hậu_duệ    phụ_huynh           0.532354          0           0\n",
            "16            bệ          đầu           0.493301          0           0\n",
            "17      nhiệm_kì     nhiệm_kỳ           0.921997          1           1\n",
            "18          sáng     bóng_tối           0.562698          0           0\n",
            "19           chú         thím           0.694969          0           0\n",
            "20          báng           bá           0.597448          1           1\n",
            "21   khuyết_điểm      ưu_điểm           0.696237          0           0\n",
            "22       nội_qui      nội_quy           0.832989          1           1\n",
            "23       dĩ_vãng    tương_lai           0.586851          0           0\n",
            "24     thiếu_sót  khuyết_điểm           0.836318          1           1\n",
            "25      trái_đất      địa_cầu           0.745785          1           1\n",
            "26            vú           tí           0.513020          1           1\n",
            "27           bác           gì           0.496810          0           0\n",
            "28        đàn_bà     nam_giới           0.637489          0           0\n",
            "29     thuế_khoá      thuế_má           0.711404          1           1\n",
            "30      nữ_hoàng          vua           0.740034          0           0\n",
            "31           bác         thím           0.692840          0           0\n",
            "32        tàn_dư     tàn_tích           0.806894          1           1\n",
            "33           nam         trai           0.569523          1           1\n",
            "34      thiếu_nữ   chàng_trai           0.853786          0           0\n",
            "35     sở_trường   nhược_điểm           0.661951          0           0\n",
            "36      kết_thúc     khởi_đầu           0.766508          0           0\n",
            "37        đàn_bà          nam           0.519688          0           0\n",
            "38           hạn      vận_hạn           0.665964          1           1\n",
            "39       bồ_bịch    tình_nhân           0.621635          1           1\n",
            "40       tinh_mơ    hoàng_hôn           0.667947          0           0\n",
            "41      khởi_đầu     chấm_hết           0.698189          0           0\n",
            "42     hiện_diện         vắng           0.601656          0           0\n",
            "43            mộ           mả           0.664529          1           1\n",
            "44           ven           vệ           0.615390          1           1\n",
            "45      thân_xác      thể_xác           0.753959          1           1\n",
            "46        phụ_nữ     con_trai           0.641575          0           0\n",
            "47           tàu          tầu           0.806023          1           1\n",
            "48     tiểu_phẫu     đại_phẫu           0.841562          0           0\n",
            "49          thầy           me           0.491545          0           0\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "test_file = '/content/Word-Similarity/datasets/ViCon-400/400_noun_pairs.txt'\n",
        "#data = f = open(data_file, \"r\")\n",
        "import csv\n",
        "k = 0\n",
        "syn_ant = {\n",
        "    \"ANT\": 0,\n",
        "    \"SYN\": 1\n",
        "}\n",
        "n_compair = 1\n",
        "test_data = []\n",
        "vec = []\n",
        "css = []\n",
        "type_of_word = []\n",
        "dict_testset = {}\n",
        "dict_lists = {}\n",
        "p_word1 = []\n",
        "p_word2 = []\n",
        "for i in range(1,301):\n",
        "  dict_lists['l'+str(i)] = []\n",
        "with open(test_file, 'r') as fd:\n",
        "      for row in fd:\n",
        "        if k >= 1:\n",
        "          split_data = row.split('\\t')\n",
        "          try: #try if dictionary have that word\n",
        "            word1 = split_data[0].strip()\n",
        "            word2 = split_data[1].strip()\n",
        "            types = split_data[2].strip()\n",
        "            #print(word1, word2, syn_ant[types])\n",
        "            vec1 = np.array(word2vec[word1])\n",
        "            vec2 = np.array(word2vec[word2])\n",
        "            type_of_word.append(syn_ant[types])\n",
        "            vec12 = np.concatenate((vec1, vec2))\n",
        "            css.append(cosine_similarity_cal(vec1, vec2))\n",
        "            p_word1.append(word1)\n",
        "            p_word2.append(word2)\n",
        "            for i in range(300):\n",
        "              dict_lists['l'+str(i+1)].append(vec12[i])\n",
        "          except:\n",
        "            pass\n",
        "        k+=1\n",
        "dict_testset['Word1'] = p_word1\n",
        "dict_testset['Word2'] = p_word2\n",
        "dict_testset['CSS'] = css\n",
        "dict_testset['Type'] = type_of_word\n",
        "for i in range(300):\n",
        "  dict_testset['vec'+str(i+1)] = dict_lists['l'+str(i+1)]\n",
        "df = pd.DataFrame(dict_testset)\n",
        "df_only_input = df.drop(['Word1','Word2','Type'], axis=1)\n",
        "# create test data\n",
        "test = df_only_input.to_numpy()\n",
        "test_labels = df['Type'].to_numpy()\n",
        "# prediction\n",
        "predicted_y = clf.predict(test[:50])\n",
        "#print(X_val)\n",
        "#print(predicted_y)\n",
        "#Print output\n",
        "print(\"Accuracy:\")\n",
        "print(\"___________________________________________________________________\")\n",
        "print(metrics.classification_report(test_labels[:50], predicted_y))\n",
        "output_dict = {\"Word1\" : p_word1[:50],\n",
        "               \"Word2\" : p_word2[:50],\n",
        "               \"Cosine Similarity\": css[:50],\n",
        "               \"Predicted\": predicted_y,\n",
        "               \"True Value\": type_of_word[:50]\n",
        "               }\n",
        "out_test = pd.DataFrame(output_dict)\n",
        "print(\"____________________________________________________________________\")\n",
        "print(\"1 is Synonym and 0 is Antonym, Output test Visualize:\")\n",
        "print(\"____________________________________________________________________\")\n",
        "print(out_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PorpkMm2n3YT"
      },
      "source": [
        "**Implement the MLP from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TAZTkZWLg8LK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "class MultiLayerPerceptron():\n",
        "  def __init__(self, sizes, epochs=10, l_rate=0.001):\n",
        "    self.sizes = sizes\n",
        "    self.epochs = epochs\n",
        "    self.l_rate = l_rate\n",
        "\n",
        "    self.params = self.initialization()\n",
        "  \n",
        "  def sigmoid(self, x, der=False):\n",
        "    if der:\n",
        "      return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "    return 1/(1+np.exp(-x))\n",
        "  \n",
        "  def softmax(self, x, der=False):\n",
        "        exps = np.exp(x - x.max())\n",
        "        if der:\n",
        "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "  def ReLu(self, x, der=False):\n",
        "    if der:\n",
        "      return np.heaviside(x, 1)\n",
        "    return np.maximum(x, 0)\n",
        "    \n",
        "  def initialization(self):\n",
        "    input_layer = self.sizes[0]\n",
        "    hidden_1 = self.sizes[1]\n",
        "    hidden_2 = self.sizes[2]\n",
        "    output_layer = self.sizes[3]\n",
        "\n",
        "    params = {\n",
        "        'W1':np.random.rand(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "        'W2':np.random.rand(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "        'W3':np.random.rand(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "    }\n",
        "\n",
        "    return params\n",
        "\n",
        "  def forward_pass(self, x_train):\n",
        "    params = self.params\n",
        "\n",
        "    params['A0'] = x_train\n",
        "\n",
        "    params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "    params['A1'] = self.ReLu(params['Z1'])\n",
        "\n",
        "    params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "    params['A2'] = self.ReLu(params['Z2'])\n",
        "\n",
        "    params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "    params['A3'] = self.sigmoid(params['Z3'])\n",
        "\n",
        "    return params['A3']\n",
        "\n",
        "  def backward_pass(self, y_train, output):\n",
        "    params = self.params\n",
        "    change_w = {}\n",
        "\n",
        "    # Calculate W3 update\n",
        "    error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], der=True)\n",
        "    change_w['W3'] = np.outer(error, params['A2'])\n",
        "\n",
        "    # Calculate W2 update\n",
        "    error = np.dot(params['W3'].T, error) * self.ReLu(params['Z2'], der=True)\n",
        "    change_w['W2'] = np.outer(error, params['A1'])\n",
        "\n",
        "    # Calculate W1 update\n",
        "    error = np.dot(params['W2'].T, error) * self.ReLu(params['Z1'], der=True)\n",
        "    change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "    return change_w\n",
        "\n",
        "  def update_network_parameters(self, changes_to_w):\n",
        "    for key, value in changes_to_w.items():\n",
        "            self.params[key] -= self.l_rate * value\n",
        "  \n",
        "  def compute_accuracy(self, x_val, y_val):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_val, y_val):\n",
        "      output = self.forward_pass(x)\n",
        "      pred = np.argmax(output)\n",
        "      predictions.append(pred == np.argmax(y))\n",
        "    \n",
        "    return np.mean(predictions)\n",
        "  \n",
        "  def train(self, x_train, y_train, x_val, y_val):\n",
        "    start_time = time.time()\n",
        "    for iteration in range(self.epochs):\n",
        "      for x, y in zip(x_train, y_train):\n",
        "        output = self.forward_pass(x)\n",
        "        changes_to_w = self.backward_pass(y, output)\n",
        "        self.update_network_parameters(changes_to_w)\n",
        "      \n",
        "      accuracy = self.compute_accuracy(x_val, y_val)\n",
        "      print('Epoch: {0}, Time spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "          iteration+1, time.time() - start_time, accuracy * 100\n",
        "      ))\n",
        "    \n",
        "  def predict(self, x_test):\n",
        "    predictions = []\n",
        "    for x in (x_test):\n",
        "      output = self.forward_pass(x)\n",
        "      predictions.append(np.argmax(output))\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBfYe4ROog_I"
      },
      "source": [
        "**Training . . .**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yicjAO7ogby",
        "outputId": "42d6f9ec-92e2-4989-be95-d9286be88de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time spent: 3.18s, Accuracy: 84.62%\n",
            "Epoch: 2, Time spent: 4.60s, Accuracy: 82.79%\n",
            "Epoch: 3, Time spent: 6.02s, Accuracy: 77.33%\n",
            "Epoch: 4, Time spent: 7.44s, Accuracy: 72.58%\n",
            "Epoch: 5, Time spent: 8.83s, Accuracy: 70.92%\n",
            "Epoch: 6, Time spent: 10.22s, Accuracy: 71.62%\n",
            "Epoch: 7, Time spent: 11.60s, Accuracy: 67.12%\n",
            "Epoch: 8, Time spent: 13.01s, Accuracy: 69.88%\n",
            "Epoch: 9, Time spent: 14.48s, Accuracy: 73.96%\n",
            "Epoch: 10, Time spent: 15.89s, Accuracy: 71.88%\n",
            "Epoch: 11, Time spent: 17.25s, Accuracy: 71.17%\n",
            "Epoch: 12, Time spent: 18.67s, Accuracy: 70.12%\n",
            "Epoch: 13, Time spent: 20.03s, Accuracy: 70.38%\n",
            "Epoch: 14, Time spent: 21.42s, Accuracy: 70.25%\n",
            "Epoch: 15, Time spent: 22.79s, Accuracy: 71.17%\n",
            "Epoch: 16, Time spent: 24.24s, Accuracy: 70.17%\n",
            "Epoch: 17, Time spent: 25.61s, Accuracy: 70.29%\n",
            "Epoch: 18, Time spent: 26.98s, Accuracy: 70.29%\n",
            "Epoch: 19, Time spent: 28.32s, Accuracy: 72.08%\n",
            "Epoch: 20, Time spent: 29.69s, Accuracy: 72.50%\n",
            "Epoch: 21, Time spent: 31.06s, Accuracy: 72.71%\n",
            "Epoch: 22, Time spent: 32.41s, Accuracy: 72.42%\n",
            "Epoch: 23, Time spent: 33.77s, Accuracy: 72.54%\n",
            "Epoch: 24, Time spent: 35.17s, Accuracy: 72.71%\n",
            "Epoch: 25, Time spent: 36.53s, Accuracy: 72.25%\n",
            "Epoch: 26, Time spent: 37.95s, Accuracy: 72.25%\n",
            "Epoch: 27, Time spent: 39.30s, Accuracy: 72.33%\n",
            "Epoch: 28, Time spent: 40.68s, Accuracy: 72.25%\n",
            "Epoch: 29, Time spent: 42.09s, Accuracy: 72.25%\n",
            "Epoch: 30, Time spent: 43.45s, Accuracy: 72.33%\n",
            "Epoch: 31, Time spent: 44.90s, Accuracy: 72.08%\n",
            "Epoch: 32, Time spent: 46.36s, Accuracy: 72.46%\n",
            "Epoch: 33, Time spent: 47.74s, Accuracy: 72.67%\n",
            "Epoch: 34, Time spent: 49.13s, Accuracy: 72.92%\n",
            "Epoch: 35, Time spent: 50.53s, Accuracy: 73.12%\n",
            "Epoch: 36, Time spent: 51.93s, Accuracy: 73.12%\n",
            "Epoch: 37, Time spent: 53.31s, Accuracy: 73.50%\n",
            "Epoch: 38, Time spent: 54.74s, Accuracy: 73.46%\n",
            "Epoch: 39, Time spent: 56.17s, Accuracy: 73.71%\n",
            "Epoch: 40, Time spent: 57.54s, Accuracy: 73.54%\n",
            "Epoch: 41, Time spent: 58.94s, Accuracy: 73.88%\n",
            "Epoch: 42, Time spent: 60.34s, Accuracy: 73.79%\n",
            "Epoch: 43, Time spent: 61.74s, Accuracy: 73.92%\n",
            "Epoch: 44, Time spent: 63.13s, Accuracy: 73.92%\n",
            "Epoch: 45, Time spent: 64.49s, Accuracy: 74.21%\n",
            "Epoch: 46, Time spent: 65.92s, Accuracy: 74.04%\n",
            "Epoch: 47, Time spent: 67.26s, Accuracy: 74.25%\n",
            "Epoch: 48, Time spent: 68.61s, Accuracy: 74.17%\n",
            "Epoch: 49, Time spent: 69.99s, Accuracy: 74.08%\n",
            "Epoch: 50, Time spent: 71.35s, Accuracy: 74.04%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "mlp = MultiLayerPerceptron(sizes=[301, 64, 32, 2], epochs = 50, l_rate=0.005)\n",
        "mlp.train(X_train, Y_train, X_val, Y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict and compute accuracy:**"
      ],
      "metadata": {
        "id": "F4JK7n8FdXNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict = mlp.predict(test[:10])\n",
        "output_dict = {\"Word1\" : p_word1[:10],\n",
        "               \"Word2\" : p_word2[:10],\n",
        "               \"Cosine Similarity\": css[:10],\n",
        "               \"Predicted\": predict,\n",
        "               \"True Value\": type_of_word[:10]\n",
        "               }\n",
        "out_test = pd.DataFrame(output_dict)\n",
        "print(metrics.classification_report(out_test['True Value'], predict))\n",
        "print(\"____________________________________________________________________\")\n",
        "print(\"1 is Synonym and 0 is Antonym, Output test Visualize:\")\n",
        "print(\"____________________________________________________________________\")\n",
        "print(out_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B3uDyetdWy3",
        "outputId": "3147c9a9-d7ee-4141-9923-4a98b044bbb8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.75      0.80         8\n",
            "           1       0.33      0.50      0.40         2\n",
            "\n",
            "    accuracy                           0.70        10\n",
            "   macro avg       0.60      0.62      0.60        10\n",
            "weighted avg       0.75      0.70      0.72        10\n",
            "\n",
            "____________________________________________________________________\n",
            "1 is Synonym and 0 is Antonym, Output test Visualize:\n",
            "____________________________________________________________________\n",
            "          Word1      Word2  Cosine Similarity  Predicted  True Value\n",
            "0     khoái_lạc    nỗi_đau           0.622883          1           0\n",
            "1       yếu_kém   sức_mạnh           0.585686          1           0\n",
            "2    thanh_danh     ô_nhục           0.544493          0           0\n",
            "3           dây        dọc           0.570831          0           1\n",
            "4            ba         me           0.475859          0           0\n",
            "5          ngày    ban_đêm           0.576550          0           0\n",
            "6      nhân_tài   tài_năng           0.769763          1           1\n",
            "7      khởi_đầu  cuối_cùng           0.573943          0           0\n",
            "8       hậu_duệ      bố_mẹ           0.516518          0           0\n",
            "9  khuyết_thiếu    ưu_điểm           0.556300          0           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result is very bad!!! =>>>>> Underfitting: Model Cannot Fitting**"
      ],
      "metadata": {
        "id": "wUjBUlqYfHPo"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}