# -*- coding: utf-8 -*-
"""VuHoangLong_19020036_TranLongDung_19021245.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iuazxOY1nj5ybSwpur_GoEWoF4di4ge2

# NLP: INT3406E_41 (2022) - PGS.TS: Nguyen Phuong Thai

---


PROGRAMMING ASSIGNMENT 1: WORD SIMILARITY AND SEMANTIC RELATION CLASSIFICATION
#Vũ Hoàng Long 19020036
#Trần Long Dũng 19021245
"""

!pip install gdown
!git clone https://github.com/NLP-Projects/Word-Similarity.git

"""# EX1: Cosine similarity:
Given pre-trained embeddings of Vietnamese words, implement a function for calculating cosine similarity between word pairs. Test your program using word pairs in ViSim-400 dataset (in directory Datasets/ViSim-400).

**Dowload pretrained Word2vec Dataset:**
Link download pre-trained word2vec model 
https://drive.google.com/open?id=1z1IDKaZuJXw5g7Yebr1GDq2UfvVR_aGx
"""

!gdown https://drive.google.com/uc?id=1z1IDKaZuJXw5g7Yebr1GDq2UfvVR_aGx

"""**Extract pretrained word2vec data into a Dictionary for searching embedded vectors by key work**"""

import csv

vec_data_file = '/content/W2V_150.txt' #define path of data file
def word2vec_dictionary(data_file):
  emb_vec_dict = {}
  with open(data_file, 'r') as fd: #read data
      reader = csv.reader(fd)
      for row in reader:
        split_data = row[0].split(' ') #split data by column
        word = split_data[0]
        emb_vec = [] #create new list for containing all the 150 vectors
        for v in split_data[2:-1]: #split 150 column of vectors
          emb_vec.append(float(v)) #parsing the string vector to float number and add to list
        emb_vec_dict[word] = emb_vec #add key word and value of 150 vectors of that word into dictionary
  return emb_vec_dict
word2vec = word2vec_dictionary(vec_data_file) #create new dictionary object

word = 'đẹp_trai'
print(word2vec[word])
print(len(word2vec[word]))

"""**Define the cosine_similarity function for calculating the cosine similarity between 2 vecs:**"""

import numpy as np
from scipy import spatial

a = [1,2,3,4]
b = [5,6,7,8]
def cosine_similarity_cal(v1, v2):
  return (2 - spatial.distance.cosine(v1, v2))/2
print(cosine_similarity_cal(a,b))

"""**Calculate the cosine similarity between 2 word by vec:**"""

data_file = '/content/Word-Similarity/datasets/ViSim-400/Visim-400.txt'
#data = f = open(data_file, "r")
import csv
i = 0
n_compair = 50
with open(data_file, 'r') as fd:
    reader = csv.reader(fd)
    for row in reader:
      split_data = row[0].split('\t')
      word1 = split_data[0]
      word2 = split_data[1]
      try:
        vec1 = np.array(word2vec[word1])
        vec2 = np.array(word2vec[word2])
        print("Pair-Word: {} - {}, Cosine-similarity: {}".format(word1, word2, cosine_similarity_cal(vec1, vec2)))
      except:
        print('This compair word was not found in dictionary!')
      i+=1
      if i == n_compair:
        break

"""# EX2: K-nearest words: Given a word w, find k most-similar words of w using the function implemented in question 1.

**Find k nearest text function:**
"""

import pandas as pd
def k_nearest_neighbor(w, v, k = 1):
  similar_vec = [] #create a similar vec list for choosing a nearest vector
  map_v2w = {} #create a dictionary for mapping the vector to word after find the k nearest
  output = {}
  for i in range(len(v)):
    w_vec = word2vec[w] #search the embedding vector of the word
    word_vec = word2vec[v[i]] #search the embedding vector of the word
    csc = cosine_similarity_cal(w_vec, word_vec) #calculate the cosine similarity
    similar_vec.append(csc) #append similar score to list for searching the nearest
    map_v2w[csc] = v[i] #adding the key: vector and value: word to dictionary for quering the word which is nearest
  similar_vec.sort() #sorting by ascending order
  k_nearest_vec = similar_vec[-k:] #quering the k-nearest vector
  for vec in k_nearest_vec:
    word = map_v2w[vec] #mapping the k-nearest vector to word
    output[word] = vec
  return output
#Testing function
k = 10
w = "báo_đốm"
v = ["nước_lớn", "tiền", "nhà_thi_đấu", "kết_duyên", "mèo", "xiếc", "nguy_hiểm", "sóng", 
     "hổ", "sư_tử", "móng_vuốt","bộ_lông", "cá", "ngựa", "nhà_nước"]
print("Input w word: {}".format(w))
df = pd.DataFrame(k_nearest_neighbor(w, v, k), index=[0])
print(df)

"""# EX3: Synonym-antonym classification: Implement a Logistic Regression classifier or a Multi-layer Perceptron classifier for distinguishing synonyms and antonyms.

**Extract 301 Vector: (150 each word) and Sim Score (1 each pair-word) of Synonym and Antonym dataset**
"""

import pandas as pd

ant_file = '/content/Word-Similarity/antonym-synonym set/Antonym_vietnamese.txt'
syn_file = '/content/Word-Similarity/antonym-synonym set/Synonym_vietnamese.txt'

css = []
type_of_word = []
dict_dataset = {}
dict_lists = {}
for i in range(1,301):
  dict_lists['l'+str(i)] = []
with open(ant_file, 'r') as fd:
    for row in fd: 
      split_word = row.split(' ')
      try: #try if dictionary have that word
        word1 = split_word[0].strip()
        word2 = split_word[1].strip()
        vec1 = np.array(word2vec[word1])
        vec2 = np.array(word2vec[word2])
        vec12 = np.concatenate((vec1, vec2))
        css.append(cosine_similarity_cal(vec1, vec2))
        type_of_word.append(0)
        for i in range(300):
            dict_lists['l'+str(i+1)].append(vec12[i])
      except:
        pass
with open(syn_file, 'r') as fd:
    for row in fd:
      split_word = row.split(' ')
      try: #try if dictionary have that word
        word1 = split_word[0].strip()
        word2 = split_word[1].strip()
        vec1 = word2vec[word1]
        vec2 = word2vec[word2]
        vec12 = np.concatenate((vec1, vec2))
        css.append(cosine_similarity_cal(vec1, vec2))
        type_of_word.append(1)
        for i in range(300):
            dict_lists['l'+str(i+1)].append(vec12[i])
      except:
        pass
dict_dataset['CSS'] = css
dict_dataset['Type'] = type_of_word
for i in range(300):
  dict_dataset['vec'+str(i+1)] = dict_lists['l'+str(i+1)]
df = pd.DataFrame(dict_dataset)
df = df.reindex(np.random.permutation(df.index))

df_no_type = df.drop('Type', axis=1)
features = df_no_type.to_numpy()
labels = df['Type'].to_numpy()

print(df)

df.to_csv('Word_pair_by_Vec_Data.csv', sep='\t')

"""**Split data for training and testing**"""

from sklearn.model_selection import train_test_split

X_train, X_val, Y_train, Y_val = train_test_split(features, labels, test_size=0.30)
print('Training records:', Y_train.size)
print('Val records:', Y_val.size)

"""**Testing training with sklearn nn MLP classifier**"""

# Import MLPClassifer 
from sklearn.neural_network import MLPClassifier

# Create model object
clf = MLPClassifier(hidden_layer_sizes=(64,32),
                    random_state=1,
                    verbose=True,
                    learning_rate_init=0.01)

# Fit data onto the model
clf.fit(X_train,Y_train)

"""**TEST THE MODEL WITH VISIM400 and print OUTPUT**"""

from sklearn import metrics
test_file = '/content/Word-Similarity/datasets/ViCon-400/400_noun_pairs.txt'
#data = f = open(data_file, "r")
import csv
k = 0
syn_ant = {
    "ANT": 0,
    "SYN": 1
}
n_compair = 1
test_data = []
vec = []
css = []
type_of_word = []
dict_testset = {}
dict_lists = {}
p_word1 = []
p_word2 = []
for i in range(1,301):
  dict_lists['l'+str(i)] = []
with open(test_file, 'r') as fd:
      for row in fd:
        if k >= 1:
          split_data = row.split('\t')
          try: #try if dictionary have that word
            word1 = split_data[0].strip()
            word2 = split_data[1].strip()
            types = split_data[2].strip()
            #print(word1, word2, syn_ant[types])
            vec1 = np.array(word2vec[word1])
            vec2 = np.array(word2vec[word2])
            type_of_word.append(syn_ant[types])
            vec12 = np.concatenate((vec1, vec2))
            css.append(cosine_similarity_cal(vec1, vec2))
            p_word1.append(word1)
            p_word2.append(word2)
            for i in range(300):
              dict_lists['l'+str(i+1)].append(vec12[i])
          except:
            pass
        k+=1
dict_testset['Word1'] = p_word1
dict_testset['Word2'] = p_word2
dict_testset['CSS'] = css
dict_testset['Type'] = type_of_word
for i in range(300):
  dict_testset['vec'+str(i+1)] = dict_lists['l'+str(i+1)]
df = pd.DataFrame(dict_testset)
df_only_input = df.drop(['Word1','Word2','Type'], axis=1)
# create test data
test = df_only_input.to_numpy()
test_labels = df['Type'].to_numpy()
# prediction
predicted_y = clf.predict(test[:50])
#print(X_val)
#print(predicted_y)
#Print output
print("Accuracy:")
print("___________________________________________________________________")
print(metrics.classification_report(test_labels[:50], predicted_y))
output_dict = {"Word1" : p_word1[:50],
               "Word2" : p_word2[:50],
               "Cosine Similarity": css[:50],
               "Predicted": predicted_y,
               "True Value": type_of_word[:50]
               }
out_test = pd.DataFrame(output_dict)
print("____________________________________________________________________")
print("1 is Synonym and 0 is Antonym, Output test Visualize:")
print("____________________________________________________________________")
print(out_test)

"""**Implement the MLP from scratch**"""

import time
class MultiLayerPerceptron():
  def __init__(self, sizes, epochs=10, l_rate=0.001):
    self.sizes = sizes
    self.epochs = epochs
    self.l_rate = l_rate

    self.params = self.initialization()
  
  def sigmoid(self, x, der=False):
    if der:
      return (np.exp(-x))/((np.exp(-x)+1)**2)
    return 1/(1+np.exp(-x))
  
  def softmax(self, x, der=False):
        exps = np.exp(x - x.max())
        if der:
            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))
        return exps / np.sum(exps, axis=0)

  def ReLu(self, x, der=False):
    if der:
      return np.heaviside(x, 1)
    return np.maximum(x, 0)
    
  def initialization(self):
    input_layer = self.sizes[0]
    hidden_1 = self.sizes[1]
    hidden_2 = self.sizes[2]
    output_layer = self.sizes[3]

    params = {
        'W1':np.random.rand(hidden_1, input_layer) * np.sqrt(1. / hidden_1),
        'W2':np.random.rand(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),
        'W3':np.random.rand(output_layer, hidden_2) * np.sqrt(1. / output_layer)
    }

    return params

  def forward_pass(self, x_train):
    params = self.params

    params['A0'] = x_train

    params['Z1'] = np.dot(params["W1"], params['A0'])
    params['A1'] = self.ReLu(params['Z1'])

    params['Z2'] = np.dot(params["W2"], params['A1'])
    params['A2'] = self.ReLu(params['Z2'])

    params['Z3'] = np.dot(params["W3"], params['A2'])
    params['A3'] = self.sigmoid(params['Z3'])

    return params['A3']

  def backward_pass(self, y_train, output):
    params = self.params
    change_w = {}

    # Calculate W3 update
    error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], der=True)
    change_w['W3'] = np.outer(error, params['A2'])

    # Calculate W2 update
    error = np.dot(params['W3'].T, error) * self.ReLu(params['Z2'], der=True)
    change_w['W2'] = np.outer(error, params['A1'])

    # Calculate W1 update
    error = np.dot(params['W2'].T, error) * self.ReLu(params['Z1'], der=True)
    change_w['W1'] = np.outer(error, params['A0'])

    return change_w

  def update_network_parameters(self, changes_to_w):
    for key, value in changes_to_w.items():
            self.params[key] -= self.l_rate * value
  
  def compute_accuracy(self, x_val, y_val):
    predictions = []
    for x, y in zip(x_val, y_val):
      output = self.forward_pass(x)
      pred = np.argmax(output)
      predictions.append(pred == np.argmax(y))
    
    return np.mean(predictions)
  
  def train(self, x_train, y_train, x_val, y_val):
    start_time = time.time()
    for iteration in range(self.epochs):
      for x, y in zip(x_train, y_train):
        output = self.forward_pass(x)
        changes_to_w = self.backward_pass(y, output)
        self.update_network_parameters(changes_to_w)
      
      accuracy = self.compute_accuracy(x_val, y_val)
      print('Epoch: {0}, Time spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(
          iteration+1, time.time() - start_time, accuracy * 100
      ))
    
  def predict(self, x_test):
    predictions = []
    for x in (x_test):
      output = self.forward_pass(x)
      predictions.append(np.argmax(output))
    return predictions

"""**Training . . .**"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

mlp = MultiLayerPerceptron(sizes=[301, 64, 32, 2], epochs = 50, l_rate=0.005)
mlp.train(X_train, Y_train, X_val, Y_val)

"""**Predict and compute accuracy:**"""

predict = mlp.predict(test[:10])
output_dict = {"Word1" : p_word1[:10],
               "Word2" : p_word2[:10],
               "Cosine Similarity": css[:10],
               "Predicted": predict,
               "True Value": type_of_word[:10]
               }
out_test = pd.DataFrame(output_dict)
print(metrics.classification_report(out_test['True Value'], predict))
print("____________________________________________________________________")
print("1 is Synonym and 0 is Antonym, Output test Visualize:")
print("____________________________________________________________________")
print(out_test)

"""**Result is very bad!!! =>>>>> Underfitting: Model Cannot Fitting**"""